import gradio as gr
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage

# 1. åˆå§‹åŒ– (å’Œä¹‹å‰ä¸€æ ·)
print("æ­£åœ¨åˆå§‹åŒ–å¤§è„‘åŒºåŸŸ...")
llm = OllamaLLM(model="qwen2.5:1.5b")
embeddings = OllamaEmbeddings(model="qwen2.5:1.5b")
vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
retriever = vectorstore.as_retriever()

# --- æ ¸å¿ƒå‡çº§ï¼šå†å²é—®é¢˜æ”¹å†™å™¨ ---
# è¿™é‡Œçš„ä»»åŠ¡æ˜¯ï¼šå¦‚æœç”¨æˆ·çš„é—®é¢˜ä¾èµ–ä¸Šä¸‹æ–‡ï¼ˆæ¯”å¦‚è¯´äº†"å®ƒ"ï¼‰ï¼Œå°±æŠŠå®ƒæ”¹å†™æˆç‹¬ç«‹é—®é¢˜ã€‚
contextualize_q_system_prompt = """
ç»™å®šä¸€æ®µèŠå¤©è®°å½•å’Œä¸€ä¸ªæœ€æ–°çš„ç”¨æˆ·é—®é¢˜ï¼Œ
å¦‚æœè¿™ä¸ªé—®é¢˜å¼•ç”¨äº†ä¸Šä¸‹æ–‡ï¼ˆæ¯”å¦‚ä½¿ç”¨äº†"å®ƒ"ã€"è¿™ä¸ª"ï¼‰ï¼Œ
è¯·æŠŠå®ƒæ”¹å†™æˆä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ï¼Œä½¿å…¶ä¸éœ€è¦ä¸Šä¸‹æ–‡ä¹Ÿèƒ½è¢«ç†è§£ã€‚
ä¸è¦å›ç­”é—®é¢˜ï¼Œåªè´Ÿè´£æ”¹å†™é—®é¢˜ã€‚å¦‚æœä¸éœ€è¦æ”¹å†™ï¼ŒåŸæ ·è¾“å‡ºã€‚
"""

contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

# è¿™ä¸ªé“¾æ¡çš„ä½œç”¨ï¼š(å†å² + æ–°é—®é¢˜) -> æ”¹å†™åçš„é—®é¢˜
history_aware_retriever = contextualize_q_prompt | llm | StrOutputParser()

# --- é—®ç­”é“¾ (QA Chain) ---
qa_system_prompt = """
ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
å¦‚æœä¸çŸ¥é“ï¼Œå°±è¯šå®åœ°è¯´ä¸çŸ¥é“ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}
"""

qa_prompt = ChatPromptTemplate.from_messages([
    ("system", qa_system_prompt),
    MessagesPlaceholder("chat_history"), # æŠŠå†å²ä¹Ÿå¡è¿›å›ç­”çš„ Prompt é‡Œ
    ("human", "{input}"),
])

# å®šä¹‰ RAG é€»è¾‘ï¼š
# 1. å…ˆç”¨ retrieve æ‰¾èµ„æ–™ï¼ˆæ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è¿˜æ²¡æŠŠæ”¹å†™å™¨ä¸²è”è¿›å»ï¼Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åœ¨å‡½æ•°é‡Œæ‰‹åŠ¨ä¸²è”ï¼‰
# LangChain çš„å®Œæ•´å†™æ³•æ¯”è¾ƒå¤æ‚ï¼Œä¸ºäº†è®©ä½ çœ‹æ‡‚ï¼Œæˆ‘ä»¬ç”¨â€œå‡½æ•°å¼â€å†™æ³•

def chat_logic(message, history):
    # history æ˜¯ Gradio ä¼ è¿‡æ¥çš„åˆ—è¡¨ï¼š[['ç”¨æˆ·é—®1', 'AIç­”1'], ['ç”¨æˆ·é—®2', 'AIç­”2']]
    # æˆ‘ä»¬è¦æŠŠå®ƒè½¬æ¢æˆ LangChain è®¤è¯†çš„æ ¼å¼
    langchain_history = []
    for human_msg, ai_msg in history:
        langchain_history.append(HumanMessage(content=human_msg))
        langchain_history.append(AIMessage(content=ai_msg))
    
    # æ­¥éª¤ 1: æ”¹å†™é—®é¢˜ (è§£å†³"å®ƒ"çš„é—®é¢˜)
    # åªæœ‰å½“æœ‰å†å²è®°å½•æ—¶æ‰éœ€è¦æ”¹å†™
    if langchain_history:
        print(f"ğŸ‘€ æ­£åœ¨åˆ†æå†å²ä¸Šä¸‹æ–‡...")
        reformulated_question = history_aware_retriever.invoke({
            "chat_history": langchain_history,
            "input": message
        })
        print(f"ğŸ”„ é—®é¢˜å·²æ”¹å†™ä¸º: {reformulated_question}")
    else:
        reformulated_question = message

    # æ­¥éª¤ 2: æ‹¿ç€æ”¹å†™åçš„é—®é¢˜å»æ£€ç´¢
    docs = retriever.invoke(reformulated_question)
    context_text = "\n\n".join([d.page_content for d in docs])

    # æ­¥éª¤ 3: ç”Ÿæˆå›ç­”
    # æˆ‘ä»¬ç›´æ¥æŠŠ context å’Œ history å¡ç»™ LLM
    final_prompt = qa_prompt.format(
        context=context_text,
        chat_history=langchain_history,
        input=message
    )
    
    response = llm.invoke(final_prompt)
    return response

# --- å¯åŠ¨ç•Œé¢ ---
print("å…¨åŠŸèƒ½ AI åŠ©æ‰‹å·²å°±ç»ªï¼")
gr.ChatInterface(
    fn=chat_logic,
    title="Thevilxx çš„å…¨åŠŸèƒ½ RAG (å¸¦è®°å¿†ç‰ˆ)",
    description="æˆ‘å·²ç»æ²»å¥½äº†å¤±å¿†ç—‡ï¼Œä½ å¯ä»¥å°è¯•é—®æˆ‘ï¼š'æ˜¾å¡æ˜¯ä»€ä¹ˆï¼Ÿ' ç„¶åè¿½é—® 'å®ƒæœ‰å¤šå°‘æ˜¾å­˜ï¼Ÿ'",
    theme="ocean"
).launch()